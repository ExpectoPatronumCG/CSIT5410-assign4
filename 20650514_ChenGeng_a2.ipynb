{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIT 5800 Introduction to Big Data\n",
    "## Spring 2020\n",
    "### Assignment 2 - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "In this assignment, you will have an opportunity to:\n",
    "<ul>\n",
    "<li>apply data pre-processing tecniques that you learned in the class to a problem using Spark</li>\n",
    "<li>apply machine learning techniques that you learned in the class to a problem using Spark</li>\n",
    "</ul>\n",
    "\n",
    "<br/>\n",
    "To get started on this assignment, you need to download the given dataset and read the description carefully written on this page. Please note that all implementation of your program should be done with Python.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intended Learning Outcomes\n",
    "\n",
    "- Upon completion of this assignment, you should be able to:\n",
    "<ol>\n",
    "    <li>Demonstrate your understanding on how to pre-process data using the algorithms / techniques as described in the class.</li>\n",
    "    <li>Demonstrate your understanding on how to do prediction using the machine learning algorithms / techniques as described in the class.</li>\n",
    "    <li>Using PySpark to construct Python program to pre-process data, performing machine learning from the training data and do data classification for the testing set.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset contains daily weather observations from numerous Australian weather stations.\n",
    "The problem is to predict whether or not it will rain tomorrow by training a binary classification model on target RainTomorrow\n",
    "The target variable RainTomorrow means: Will it rain the next day? Yes or No.\n",
    "\n",
    "Note: You should exclude the variable Risk-MM when training a binary classification model. Not excluding it will leak the answers to your model and reduce its predictability. Read more about it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Note: The suggested functions below are for reference only. You can use any functions from PySpark.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Installing PySpark\n",
    "### Step 0.1 : Install Java\n",
    "#### Step 0.1.1: Check Java Version\n",
    "\n",
    "In command prompt:\n",
    "<pre>java -version</pre>\n",
    "\n",
    "Note: PySpark requires Java version 7 or later.\n",
    "\n",
    "#### Step 0.1.2\n",
    "Install java from the official download website.\n",
    "<url> https://www.oracle.com/java/technologies/javase-jdk8-downloads.html </url>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.2: Install Apache Spark on Windows\n",
    "<ol>\n",
    "    <li>Go to the <a href=\"http://spark.apache.org/downloads.html\">Spark download</a>.</li>\n",
    "    <li>Select the latest stable release (2.4.5 as of May-2020) of Spark for \"Choose a Spark release\".</li>\n",
    "    <li>Select a version that is pre-built for the latest version of Hadoop such as Pre-built for Hadoop 2.7 and later\n",
    "        for \"Choose a package type\"</li>\n",
    "    <li>Click the link next to Download Spark to download the spark-2.4.5-bin-hadoop2.7.tgz</li>\n",
    "    <li>Extract the files from the downloaded zip file using winzip or equivalent (right click on the extracted file and click extract here).</li>\n",
    "    <li>Make sure that the folder path and the folder name containing Spark files do not contain any spaces.</li>\n",
    "    <li>Create a folder called \"spark\" on your desktop and unzip the file that you downloaded as a folder called spark-2.4.5-bin-hadoop2.7. So, all Spark files will be in a folder called C:\\Users\\[your_user_name]\\Desktop\\Spark\\spark-2.4.0-bin-hadoop2.7. This will be referred as SPARK_HOME.</li>\n",
    "    <li>To test if your installation was successful, open Anaconda Prompt, change to SPARK_HOME directory and type bin\\pyspark. This should start the PySpark shell which can be used to interactively work with Spark. </li>\n",
    "    <li>Create a system environment variable in Windows called SPARK_HOME that points to the SPARK_HOME folder path.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.2: Install Apache Spark on Mac\n",
    "\n",
    "You can use Homebrew to install Apache Spark.\n",
    "<ol>\n",
    "    <li>Install Homebrew using the following command in your terminal:<br/>\n",
    "        <pre>/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"</pre></li>\n",
    "    <li>Install Spark with Homebrew\n",
    "         <ol>\n",
    "         <li>In your terminal, type: <br />\n",
    "         <pre>brew install apache-spark</pre></li>\n",
    "         <li>You can check the version of spark: <br />\n",
    "         <pre>pyspark â€“version</pre></li>\n",
    "         </ol>\n",
    "    </li>\n",
    "    <li>You may need to install PySpark by:<br />\n",
    "    <pre>pip install pyspark</pre>\n",
    "    </li>\n",
    "    <li>To know where spark is installed: <br/>\n",
    "    <pre>brew info apache-spark</pre>\n",
    "    </li>\n",
    "    <li>Set the environment variables:<br />\n",
    "    <pre>export SPARK_HOME=\"[your_path]/ibexec/\"</pre>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.3 Using Pyspark in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "findspark is a library that automatically sets up the development environment to import Apache Spark library.\n",
    "To install findspark, run the following in your shell:<br />\n",
    "<pre>pip install findspark</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib/spark/spark-3.0.0-preview2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 1500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing data and exploring the features (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1\n",
    "Read the csv file 'weatherAUS.csv' using \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.SparkSession\">SparkSession.read()</a> to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "data= spark.read.csv(path='weatherAUS.csv',inferSchema=True,header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2\n",
    "Use show() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to print out the schema of the dataframe created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|      Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RISK_MM|RainTomorrow|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "|2008-12-01|  Albury|   13.4|   22.9|     0.6|         NA|      NA|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       8|      NA|   16.9|   21.8|       No|    0.0|          No|\n",
      "|2008-12-02|  Albury|    7.4|   25.1|       0|         NA|      NA|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|      NA|      NA|   17.2|   24.3|       No|    0.0|          No|\n",
      "|2008-12-03|  Albury|   12.9|   25.7|       0|         NA|      NA|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|      NA|       2|     21|   23.2|       No|    0.0|          No|\n",
      "|2008-12-04|  Albury|    9.2|     28|       0|         NA|      NA|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|      NA|      NA|   18.1|   26.5|       No|    1.0|          No|\n",
      "|2008-12-05|  Albury|   17.5|   32.3|       1|         NA|      NA|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|       1006|       7|       8|   17.8|   29.7|       No|    0.2|          No|\n",
      "|2008-12-06|  Albury|   14.6|   29.7|     0.2|         NA|      NA|        WNW|           56|         W|         W|          19|          24|         55|         23|     1009.2|     1005.4|      NA|      NA|   20.6|   28.9|       No|    0.0|          No|\n",
      "|2008-12-07|  Albury|   14.3|     25|       0|         NA|      NA|          W|           50|        SW|         W|          20|          24|         49|         19|     1009.6|     1008.2|       1|      NA|   18.1|   24.6|       No|    0.0|          No|\n",
      "|2008-12-08|  Albury|    7.7|   26.7|       0|         NA|      NA|          W|           35|       SSE|         W|           6|          17|         48|         19|     1013.4|     1010.1|      NA|      NA|   16.3|   25.5|       No|    0.0|          No|\n",
      "|2008-12-09|  Albury|    9.7|   31.9|       0|         NA|      NA|        NNW|           80|        SE|        NW|           7|          28|         42|          9|     1008.9|     1003.6|      NA|      NA|   18.3|   30.2|       No|    1.4|         Yes|\n",
      "|2008-12-10|  Albury|   13.1|   30.1|     1.4|         NA|      NA|          W|           28|         S|       SSE|          15|          11|         58|         27|       1007|     1005.7|      NA|      NA|   20.1|   28.2|      Yes|    0.0|          No|\n",
      "|2008-12-11|  Albury|   13.4|   30.4|       0|         NA|      NA|          N|           30|       SSE|       ESE|          17|           6|         48|         22|     1011.8|     1008.7|      NA|      NA|   20.4|   28.8|       No|    2.2|         Yes|\n",
      "|2008-12-12|  Albury|   15.9|   21.7|     2.2|         NA|      NA|        NNE|           31|        NE|       ENE|          15|          13|         89|         91|     1010.5|     1004.2|       8|       8|   15.9|     17|      Yes|   15.6|         Yes|\n",
      "|2008-12-13|  Albury|   15.9|   18.6|    15.6|         NA|      NA|          W|           61|       NNW|       NNW|          28|          28|         76|         93|      994.3|        993|       8|       8|   17.4|   15.8|      Yes|    3.6|         Yes|\n",
      "|2008-12-14|  Albury|   12.6|     21|     3.6|         NA|      NA|         SW|           44|         W|       SSW|          24|          20|         65|         43|     1001.2|     1001.8|      NA|       7|   15.8|   19.8|      Yes|    0.0|          No|\n",
      "|2008-12-16|  Albury|    9.8|   27.7|      NA|         NA|      NA|        WNW|           50|        NA|       WNW|          NA|          22|         50|         28|     1013.4|     1010.3|       0|      NA|   17.3|   26.2|       NA|    0.0|          No|\n",
      "|2008-12-17|  Albury|   14.1|   20.9|       0|         NA|      NA|        ENE|           22|       SSW|         E|          11|           9|         69|         82|     1012.2|     1010.4|       8|       1|   17.2|   18.1|       No|   16.8|         Yes|\n",
      "|2008-12-18|  Albury|   13.5|   22.9|    16.8|         NA|      NA|          W|           63|         N|       WNW|           6|          20|         80|         65|     1005.8|     1002.2|       8|       1|     18|   21.5|      Yes|   10.6|         Yes|\n",
      "|2008-12-19|  Albury|   11.2|   22.5|    10.6|         NA|      NA|        SSE|           43|       WSW|        SW|          24|          17|         47|         32|     1009.4|     1009.7|      NA|       2|   15.5|     21|      Yes|    0.0|          No|\n",
      "|2008-12-20|  Albury|    9.8|   25.6|       0|         NA|      NA|        SSE|           26|        SE|       NNW|          17|           6|         45|         26|     1019.2|     1017.1|      NA|      NA|   15.8|   23.2|       No|    0.0|          No|\n",
      "|2008-12-21|  Albury|   11.5|   29.3|       0|         NA|      NA|          S|           24|        SE|        SE|           9|           9|         56|         28|     1019.3|     1014.8|      NA|      NA|   19.1|   27.3|       No|    0.0|          No|\n",
      "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your statement here\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3\n",
    "Use printSchema() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to print out the schema of the dataframe created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1504,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RISK_MM: double (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your statement here\n",
    "data.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the features above have a wrong datatype in the schema? What should the correct datatypes be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>MinTemp float64</font>\n",
    "\n",
    "<font color=red>MaxTemp float64</font>\n",
    "\n",
    "<font color=red>Rainfall float64</font>\n",
    "\n",
    "<font color=red>Evaporation float64</font>\n",
    "\n",
    "<font color=red>Sunshine float64</font>\n",
    "\n",
    "<font color=red>WindGustSpeed float64</font>\n",
    "\n",
    "<font color=red>WindSpeed9am float64</font>\n",
    "\n",
    "<font color=red>WindSpeed3pm float64</font>\n",
    "\n",
    "<font color=red>Humidity9am float64</font>\n",
    "\n",
    "<font color=red>Humidity3pm float64</font>\n",
    "\n",
    "<font color=red>Pressure9am float64</font>\n",
    "\n",
    "<font color=red>Pressure3pm float64</font>\n",
    "\n",
    "<font color=red>Cloud9am float64</font>\n",
    "\n",
    "<font color=red>Cloud3pm float64</font>\n",
    "\n",
    "<font color=red>Temp9am float64</font>\n",
    "\n",
    "<font color=red>Temp3pm float64 </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert the data types of features (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1\n",
    "Import data types from \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#module-pyspark.sql.types\">pyspark.sql.types</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.sql.types import DataType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2\n",
    "Convert the datatype of features using:\n",
    "<ul>\n",
    "<li>withColumn() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>, and</li>\n",
    "<li>cast() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.Column\">pyspark.sql.Column</a>\n",
    "to convert the features (columns) into appropriate types \n",
    "(<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#module-pyspark.sql.types\">pyspark.sql.types</a>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "col=['MinTemp', 'MaxTemp','Rainfall','Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm', 'Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm']\n",
    "for c in col:\n",
    "    data=data.withColumn(c,data[c].cast('float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:Exploring missing values (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 \n",
    "<ul>\n",
    "<li>Count the missing values in features using:\n",
    "<ul>\n",
    "<li>isNull() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.Column\">pyspark.sql.Column</a>\n",
    "to see whether there are missing values in a certain feature (column).</li>\n",
    "<li>filter() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
    "to filter rows given a condition, and</li>\n",
    "<li>count() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
    "to count the number of rows in a dataframe</li>\n",
    "</ul>\n",
    "<li>Print the features with missing values and their corresponding number of rows with missing values</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Date : 0 \n",
      "Feature Location : 0 \n",
      "Feature MinTemp : 637 \n",
      "Feature MaxTemp : 322 \n",
      "Feature Rainfall : 1406 \n",
      "Feature Evaporation : 60843 \n",
      "Feature Sunshine : 67816 \n",
      "Feature WindGustDir : 0 \n",
      "Feature WindGustSpeed : 9270 \n",
      "Feature WindDir9am : 0 \n",
      "Feature WindDir3pm : 0 \n",
      "Feature WindSpeed9am : 1348 \n",
      "Feature WindSpeed3pm : 2630 \n",
      "Feature Humidity9am : 1774 \n",
      "Feature Humidity3pm : 3610 \n",
      "Feature Pressure9am : 14014 \n",
      "Feature Pressure3pm : 13981 \n",
      "Feature Cloud9am : 53657 \n",
      "Feature Cloud3pm : 57094 \n",
      "Feature Temp9am : 904 \n",
      "Feature Temp3pm : 2726 \n",
      "Feature RainToday : 0 \n",
      "Feature RISK_MM : 0 \n",
      "Feature RainTomorrow : 0 \n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "for col in data.columns:\n",
    "    print('Feature %s : %d '%(col,data.filter(data[col].isNull()).count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2\n",
    "Some features have values of \"NA\", which should be regarded as missing values.</br>\n",
    "Print those features with values of \"NA\" and their corresponding number of rows with values \"NA\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Date : 0 \n",
      "Feature Location : 0 \n",
      "Feature MinTemp : 0 \n",
      "Feature MaxTemp : 0 \n",
      "Feature Rainfall : 0 \n",
      "Feature Evaporation : 0 \n",
      "Feature Sunshine : 0 \n",
      "Feature WindGustDir : 9330 \n",
      "Feature WindGustSpeed : 0 \n",
      "Feature WindDir9am : 10013 \n",
      "Feature WindDir3pm : 3778 \n",
      "Feature WindSpeed9am : 0 \n",
      "Feature WindSpeed3pm : 0 \n",
      "Feature Humidity9am : 0 \n",
      "Feature Humidity3pm : 0 \n",
      "Feature Pressure9am : 0 \n",
      "Feature Pressure3pm : 0 \n",
      "Feature Cloud9am : 0 \n",
      "Feature Cloud3pm : 0 \n",
      "Feature Temp9am : 0 \n",
      "Feature Temp3pm : 0 \n",
      "Feature RainToday : 1406 \n",
      "Feature RISK_MM : 0 \n",
      "Feature RainTomorrow : 0 \n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "for col in data.columns:\n",
    "    print('Feature %s : %d '%(col,data.filter(data[col]=='NA').count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Drop the feature, RISK_MM (2 points)\n",
    "As described in the dataset description, we will need to drop the feature, RISK_MM, using drop() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "data=data.drop('RISK_MM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Processing the date feature (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1\n",
    "Import the to_date(), year(), month(), dayofmonth()\n",
    "from\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.sql.functions import to_date,year,month,dayofmonth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2\n",
    "Use the to_date() function to convert the <strong>Date</strong> attribute to datetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "data=data.withColumn('Date',to_date(data['Date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.3\n",
    "Extract the <strong>year</strong>, <strong>month</strong>, <strong>day</strong> attributes of the converted <strong>Date</strong> attribute using year(), month() & dayofmonth()\n",
    "and create the corresponding new features <strong>Year</strong>, <strong>Month</strong> and <strong>Day</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "data=data.withColumn('Year',year(data['Date']))\n",
    "data=data.withColumn('Month',month(data['Date']))\n",
    "data=data.withColumn('Day',dayofmonth(data['Date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.4\n",
    "Drop the original <strong>Date</strong> feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "data=data.drop('Date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Handling missing values of the categorical features (8 marks)\n",
    "### Step 6.1 Find the most frequent value for categorical features with \"NA\" values \n",
    "Use groupBy(), count() & show() of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a> to list the distinct values of a feature and count the corresponding number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical features with missing values (\"NA\"), what is the most frequent value of each of the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|WindGustDir|count|\n",
      "+-----------+-----+\n",
      "|          W| 9780|\n",
      "+-----------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----+\n",
      "|WindDir9am|count|\n",
      "+----------+-----+\n",
      "|         N|11393|\n",
      "+----------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----+\n",
      "|WindDir3pm|count|\n",
      "+----------+-----+\n",
      "|        SE|10663|\n",
      "+----------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------+------+\n",
      "|RainToday| count|\n",
      "+---------+------+\n",
      "|       No|109332|\n",
      "+---------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "from pyspark.sql.functions import desc\n",
    "for index in data.columns:\n",
    "    if data.select(index).dtypes[0][1]=='string' and data.filter(data[index]=='NA').count()!=0:\n",
    "        data.groupBy(index).count().sort(desc(\"count\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">WindGustDir: W</font>\n",
    "    \n",
    "<font color=\"red\">WindDir9am: N</font>\n",
    "\n",
    "<font color=\"red\">WindDir3pm: SE</font>\n",
    "\n",
    "<font color=\"red\">RainToday: No</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2\n",
    "Import when(), lit() from\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.sql.functions import when,lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3\n",
    "For each of the categorical features with missing values (\"NA\"), replace \"NA\" with the corresponding most frequent value of each of the features using when(), lit(), otherwise()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "data=data.withColumn('WindGustDir',when(data['WindGustDir']=='NA',lit('W')).otherwise(data['WindGustDir']))\n",
    "data=data.withColumn('WindDir9am',when(data['WindDir9am']=='NA',lit('N')).otherwise(data['WindDir9am']))\n",
    "data=data.withColumn('WindDir3pm',when(data['WindDir3pm']=='NA',lit('SE')).otherwise(data['WindDir3pm']))\n",
    "data=data.withColumn('RainToday',when(data['RainToday']=='NA',lit('No')).otherwise(data['RainToday']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 Handling missing values of the numerical features (10 marks)\n",
    "### Step 7.1 \n",
    "Print the list of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'Year', 'Month', 'Day']\n"
     ]
    }
   ],
   "source": [
    "# Put your statement(s) here\n",
    "num_f=[]\n",
    "for index in data.columns:\n",
    "    if data.select(index).dtypes[0][1]!='string':\n",
    "        num_f.append(index)\n",
    "print(num_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2\n",
    "Import Imputer from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml.feature import Imputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.3\n",
    "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer\">pyspark.ml.feature.imputer</a> to fill in the missing values of the numerical features with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "imputer=Imputer(strategy='mean',inputCols=num_f, outputCols=num_f)\n",
    "#for index in data.columns:\n",
    "    #if data.select(index).dtypes[0][1]!='string' and data.filter(data[index].isNull()).count()!=0:\n",
    "imputer=imputer.fit(data)\n",
    "data=imputer.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Transform the features (10 marks)\n",
    "### Step 8.1\n",
    "Import skewness from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.sql.functions import skewness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.2\n",
    "We can get the skewness using skewness() from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module.\n",
    "\n",
    "Find the features which are with skewness values larger than 0.75, and print the features together with their skewness values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|skewness(Rainfall)|\n",
      "+------------------+\n",
      "|  9.93720720656584|\n",
      "+------------------+\n",
      "\n",
      "+---------------------+\n",
      "|skewness(Evaporation)|\n",
      "+---------------------+\n",
      "|   4.9535525141524825|\n",
      "+---------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|skewness(WindGustSpeed)|\n",
      "+-----------------------+\n",
      "|     0.9042674361671061|\n",
      "+-----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|skewness(WindSpeed9am)|\n",
      "+----------------------+\n",
      "|    0.7791876023517204|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "skew_index=[]\n",
    "for index in data.columns:\n",
    "    if data.select(index).dtypes[0][1]!='string':\n",
    "        if data.select(skewness(index)).take(1)[0][-1]>0.75:\n",
    "            data.select(skewness(index)).show()\n",
    "            skew_index.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.3\n",
    "Import log1p from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.sql.functions import log1p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8.4\n",
    "\n",
    "Apply log transformation on those features with skewness values larger than 0.75 using log1p() from \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\">pyspark.sql.functions</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "for i in skew_index:\n",
    "    data=data.withColumn(i,log1p(data[i]))\n",
    "pipline_data=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 Converting Categorial features  (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.1 List categorical features\n",
    "Get and print the list of categorial features (exclude \"RainTomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "cat_f=[]\n",
    "cat_trans=[]\n",
    "for index in data.columns:\n",
    "    if data.select(index).dtypes[0][1]=='string' and index!='RainTomorrow':\n",
    "        cat_f.append(index)\n",
    "        cat_trans.append(index+'_onehot')\n",
    "print(cat_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.2 Convert categorical features into dummy/indicator features\n",
    "#### Step 9.2.1\n",
    "Import <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer\">StringIndex</a> and <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoderEstimator\">OneHotEncoderEstimator</a> from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9.2.2\n",
    "Using StringIndexer to convert categorical values into category indices for each of the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "for i in cat_f:\n",
    "    stringindexer = StringIndexer(inputCol=i, outputCol=i+'_indexer')\n",
    "    stringindexer=stringindexer.fit(data)\n",
    "    data=stringindexer.transform(data)\n",
    "    data=data.drop(i)\n",
    "    data=data.withColumnRenamed(i+'_indexer',i)\n",
    "    \n",
    "label_indexer = StringIndexer(inputCol='RainTomorrow', outputCol='RainTomorrow_num').fit(data)\n",
    "data = label_indexer.transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9.2.3\n",
    "Using OneHotEncoderEstimator to map a column of category indices to a column of binary vectors for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "encoder = OneHotEncoder(dropLast=False, inputCols=cat_f, outputCols=cat_trans)\n",
    "encoder=encoder.fit(data)\n",
    "data=encoder.transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Training the Regression model  (20 points)\n",
    "\n",
    "### Step 10.1: Creating the feature vector\n",
    "\n",
    "#### Step 10.1.1\n",
    "Import VectorAssembler from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\">pyspark.ml.feature</a> module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.1.2\n",
    "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler\">VectorAssembler<a/>, a feature transformer, to merge the columns created in step 9.2.3 and the columns of numerical features into a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "f_all=num_f[:]\n",
    "#f_all.extend(cat_trans)\n",
    "vector=VectorAssembler(inputCols=f_all,outputCol=\"features\")\n",
    "new_data=vector.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.2 \n",
    "Is there any other transformation on the dataframe needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml.feature import Normalizer\n",
    "normalizer = Normalizer(p=2.0, inputCol='features', outputCol='features_norm')\n",
    "new_data = normalizer.transform(new_data)\n",
    "new_data=new_data.drop('features')\n",
    "\n",
    "f_all=['features_norm']\n",
    "f_all.extend(cat_trans)\n",
    "vector=VectorAssembler(inputCols=f_all,outputCol=\"features\")\n",
    "new_data=vector.transform(new_data)\n",
    "new_data=new_data.drop('features_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.3 Split the data into training data and testing data\n",
    "\n",
    "Using randomSplit of <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">pyspark.sql.DataFrame</a>\n",
    "to randomly splits the DataFrame with the ratio of 0.7 and 0.3 into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "train_data,test_data=new_data.randomSplit([0.7,0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.4 Build the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.1\n",
    "Import LogisticRegression from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification\">pyspark.ml.classification</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.2\n",
    "<ol>\n",
    "<li>Initialize a Logistic Regression model by <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression\">LogisticRegression()</a> function using the feature vector generated in Step 10.1.2.</li>\n",
    "<li>Use fit() function to train the logistic regression model using the training feature data.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "blor=LogisticRegression(regParam=0.01, maxIter=1000,labelCol = 'RainTomorrow_num').fit(train_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.3\n",
    "<ul>\n",
    "    <li>Gets summary of model trained on the training set. </li>\n",
    "    <li>Print the accuracy, objective history, total iterations of the trained model.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8476166557838164\n",
      "Iterations: 302\n",
      "ObjectiveHistory:\n",
      "0.5319232091540478\n",
      "0.5312988787331828\n",
      "0.50115352804076\n",
      "0.40836677237548574\n",
      "0.4026176540546599\n",
      "0.4017958286217634\n",
      "0.4017562332761363\n",
      "0.4017525386128751\n",
      "0.40173256387834544\n",
      "0.40165202475701756\n",
      "0.4014933952570464\n",
      "0.4010430264585518\n",
      "0.3999713290324089\n",
      "0.39748484631192993\n",
      "0.392771833561181\n",
      "0.38678951981117665\n",
      "0.3847319840632798\n",
      "0.38127307365806967\n",
      "0.3802263319781325\n",
      "0.38017078166599455\n",
      "0.37970355934625016\n",
      "0.37604208896810126\n",
      "0.37342685857351826\n",
      "0.37169360102877125\n",
      "0.37149328664619086\n",
      "0.3714836963347408\n",
      "0.3714788596284995\n",
      "0.37146775010599226\n",
      "0.37143922198309975\n",
      "0.37143332059003686\n",
      "0.37137333805895323\n",
      "0.37118388670150526\n",
      "0.3708708357963599\n",
      "0.37057151521350606\n",
      "0.3694303266782195\n",
      "0.36757964596374143\n",
      "0.36618925001035135\n",
      "0.3657991142304294\n",
      "0.36575031951269965\n",
      "0.36573022101476826\n",
      "0.36572543948705194\n",
      "0.36571602053145286\n",
      "0.36555201438215235\n",
      "0.3643807248445877\n",
      "0.3641454438896196\n",
      "0.3640098674367555\n",
      "0.364004489416136\n",
      "0.3640013815231301\n",
      "0.3639985241110216\n",
      "0.3639877189011554\n",
      "0.3639641141515757\n",
      "0.36396143574240264\n",
      "0.36391056897506574\n",
      "0.36383786381729816\n",
      "0.3637926757807019\n",
      "0.3637396096554131\n",
      "0.3637090300659007\n",
      "0.3637010616107673\n",
      "0.3636243285482075\n",
      "0.3635395856530983\n",
      "0.36342233048288053\n",
      "0.3633956106917252\n",
      "0.36338534032833225\n",
      "0.36336314589528795\n",
      "0.36335758350013747\n",
      "0.36335738694098185\n",
      "0.36335701291143696\n",
      "0.36335374505422857\n",
      "0.36331922536426386\n",
      "0.363268390140152\n",
      "0.3631650317895684\n",
      "0.3630636162111007\n",
      "0.3630567907451088\n",
      "0.3630444290194165\n",
      "0.3630120049703631\n",
      "0.36300683550171897\n",
      "0.3630063098906898\n",
      "0.36300605770328115\n",
      "0.36300584415031334\n",
      "0.36300556917499976\n",
      "0.36300551629679095\n",
      "0.3630054424589656\n",
      "0.36300533375805744\n",
      "0.3630051431178739\n",
      "0.3630046206870973\n",
      "0.36300336144778866\n",
      "0.3630013327699333\n",
      "0.3629980827885657\n",
      "0.36299249694095787\n",
      "0.362991872050198\n",
      "0.3629905074349417\n",
      "0.3629885584419076\n",
      "0.36298422990108126\n",
      "0.36298255049834033\n",
      "0.362967782513139\n",
      "0.36296214358476564\n",
      "0.3629468111340549\n",
      "0.3629457422648702\n",
      "0.36293870849821136\n",
      "0.3629366784058572\n",
      "0.36293649514647935\n",
      "0.3629362266277264\n",
      "0.36293596100177183\n",
      "0.36293320801713597\n",
      "0.3629293398256794\n",
      "0.3629224833684884\n",
      "0.36291725925097557\n",
      "0.3629154190334897\n",
      "0.3629152200535824\n",
      "0.362915210288477\n",
      "0.36291520120747117\n",
      "0.36291518211307633\n",
      "0.36291510393549303\n",
      "0.3629149842120617\n",
      "0.36291356364599514\n",
      "0.3629079385300197\n",
      "0.36290005675763987\n",
      "0.36288957112255776\n",
      "0.3628893398022488\n",
      "0.3628834167896295\n",
      "0.36287166093420203\n",
      "0.362852230124211\n",
      "0.36281934051633097\n",
      "0.36281894943223464\n",
      "0.36281869611140727\n",
      "0.3628180545311385\n",
      "0.36281734935313936\n",
      "0.36281540402190904\n",
      "0.3628122263991687\n",
      "0.36281164222241585\n",
      "0.3628087921968705\n",
      "0.36280724528698316\n",
      "0.3627925168938125\n",
      "0.36278580385108017\n",
      "0.3627817475852505\n",
      "0.36278150830947875\n",
      "0.36278143516098177\n",
      "0.3627813292541916\n",
      "0.36278131846152284\n",
      "0.3627812978816376\n",
      "0.36278124472467893\n",
      "0.36278109603438136\n",
      "0.36278087810098486\n",
      "0.3627806520239639\n",
      "0.3627805552858047\n",
      "0.3627805052971746\n",
      "0.362780443775237\n",
      "0.3627803468520292\n",
      "0.36278005190112833\n",
      "0.3627794107195655\n",
      "0.3627780392457118\n",
      "0.3627762845944624\n",
      "0.36277527924927944\n",
      "0.36277290261535955\n",
      "0.3627722787928621\n",
      "0.36277207013843865\n",
      "0.36277197444948245\n",
      "0.36277190900672407\n",
      "0.3627718184966735\n",
      "0.3627715360510953\n",
      "0.3627710059497594\n",
      "0.36276987604185973\n",
      "0.3627676926125663\n",
      "0.3627636092462099\n",
      "0.36276293189436876\n",
      "0.3627566004568882\n",
      "0.36275349597127327\n",
      "0.36275233524234196\n",
      "0.36275171086402014\n",
      "0.3627497831990384\n",
      "0.36274712885734917\n",
      "0.3627470688306184\n",
      "0.36274695480062596\n",
      "0.3627460450170026\n",
      "0.3627443916441941\n",
      "0.3627401547112537\n",
      "0.3627363887551513\n",
      "0.3627270696232805\n",
      "0.3626969279302661\n",
      "0.3626879614869835\n",
      "0.3626858876642849\n",
      "0.3626848083231341\n",
      "0.3626846154637519\n",
      "0.3626844653177643\n",
      "0.36268385675487586\n",
      "0.3626829017567304\n",
      "0.3626812024071584\n",
      "0.362681155762477\n",
      "0.36267961778075225\n",
      "0.36267909622772165\n",
      "0.3626789533456258\n",
      "0.36267888358510375\n",
      "0.36267887974259755\n",
      "0.36267878841058704\n",
      "0.36267857483860816\n",
      "0.3626785273758415\n",
      "0.3626785155177755\n",
      "0.36267850049267947\n",
      "0.3626784179842121\n",
      "0.36267821002494344\n",
      "0.3626777781288729\n",
      "0.36267687375379903\n",
      "0.3626747809267336\n",
      "0.36266166815215833\n",
      "0.36266127860343406\n",
      "0.3626559821256034\n",
      "0.3626542212064673\n",
      "0.36265410179048924\n",
      "0.3626540676975591\n",
      "0.3626540021979419\n",
      "0.36265386243585596\n",
      "0.3626535453434571\n",
      "0.3626529143087386\n",
      "0.3626527370682105\n",
      "0.3626518486113768\n",
      "0.3626511745254963\n",
      "0.3626510461795118\n",
      "0.36265103714404784\n",
      "0.3626510016582222\n",
      "0.36265096579332484\n",
      "0.36265078642254756\n",
      "0.36265050562951856\n",
      "0.3626501646117776\n",
      "0.3626446275039492\n",
      "0.3626442476535567\n",
      "0.3626365596631096\n",
      "0.36263333266269987\n",
      "0.36263240050574136\n",
      "0.36263237657986946\n",
      "0.36263235583194264\n",
      "0.3626323475880956\n",
      "0.36263229832341265\n",
      "0.3626321935992079\n",
      "0.3626319303621157\n",
      "0.3626313400385494\n",
      "0.3626301320240273\n",
      "0.36262850405356223\n",
      "0.36262827759941535\n",
      "0.36262714258169804\n",
      "0.362626765215376\n",
      "0.36262673533227446\n",
      "0.3626267348490568\n",
      "0.3626267294086212\n",
      "0.3626267161900199\n",
      "0.36262664152188295\n",
      "0.36262655655620074\n",
      "0.3626264913304878\n",
      "0.36262646911403923\n",
      "0.3626264641528319\n",
      "0.3626264602607563\n",
      "0.3626264399251709\n",
      "0.36262641191478695\n",
      "0.362626340765599\n",
      "0.3626262958882718\n",
      "0.36262615703506407\n",
      "0.3626254116227679\n",
      "0.3626247566606464\n",
      "0.362623986106925\n",
      "0.3626239221568177\n",
      "0.3626235816159068\n",
      "0.36262353761261656\n",
      "0.3626235266319003\n",
      "0.362623511901259\n",
      "0.36262348697737523\n",
      "0.3626234645022705\n",
      "0.3626234478148932\n",
      "0.3626234359284635\n",
      "0.3626234336318299\n",
      "0.36262343342800285\n",
      "0.3626234278221601\n",
      "0.3626234093561397\n",
      "0.3626234053447986\n",
      "0.36262339118262105\n",
      "0.36262337827598046\n",
      "0.3626233328722468\n",
      "0.3626233182979521\n",
      "0.36262323492237974\n",
      "0.362623169292102\n",
      "0.36262292430515725\n",
      "0.36262287897299855\n",
      "0.36262274348769213\n",
      "0.3626225232098984\n",
      "0.36262232958863194\n",
      "0.36262218735333457\n",
      "0.36262216374376005\n",
      "0.36262196360459653\n",
      "0.36262192973799934\n",
      "0.36262192332118454\n",
      "0.3626219210824226\n",
      "0.3626219192674866\n",
      "0.36262191227721924\n",
      "0.36262190514041603\n",
      "0.3626219019589958\n",
      "0.36262189468258155\n",
      "0.3626218934397301\n",
      "0.36262189324913335\n",
      "0.36262189316676546\n",
      "0.3626218927891833\n",
      "0.36262189202813455\n",
      "0.3626218919727464\n",
      "0.3626218909863664\n",
      "0.3626218899808267\n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "trainingSummary = blor.summary\n",
    "print(\"Accuracy: \" + str(trainingSummary.accuracy))\n",
    "print(\"Iterations: \" + str(trainingSummary.totalIterations))\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"ObjectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.4\n",
    "Predict the target values for the testing feature data using the transform() function.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "predict=blor.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.4.5\n",
    "Use show() to list the top 5 rows of results in Step 10.4.4, show only \"prediction\", \"RainTomorrow\" and the featuresCol specified in the LogisticRegression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+------------------+--------+------------------+------------------+------------+-----------+-----------+-----------+-----------+---------+---------+-------+-------+----+-----+---+----------+----------------+\n",
      "|MinTemp|MaxTemp|          Rainfall|       Evaporation|Sunshine|     WindGustSpeed|      WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm| Cloud9am| Cloud3pm|Temp9am|Temp3pm|Year|Month|Day|prediction|RainTomorrow_num|\n",
      "+-------+-------+------------------+------------------+--------+------------------+------------------+------------+-----------+-----------+-----------+-----------+---------+---------+-------+-------+----+-----+---+----------+----------------+\n",
      "|   -3.2|   15.7|               0.0|1.4350845366425882|     9.1| 3.044522437723423|1.0986122886681096|        13.0|       71.0|       36.0|     1026.2|     1023.0|      0.0|      0.0|    5.5|   15.0|2011|    7| 12|       0.0|             0.0|\n",
      "|   -3.1|   16.9|               0.0|1.8671489542210176|7.624853|3.6888794541139363|1.0986122886681096|        26.0|       83.0|       35.0|     1020.0|     1016.1|4.4371896|4.5031667|    3.7|   16.1|2011|    7|  9|       0.0|             0.0|\n",
      "|   -3.0|   16.1|1.2089526310090144|1.8671489542210176|7.624853|3.1354942159291497|               0.0|         0.0|       84.0|       39.0|     1021.0|     1018.8|4.4371896|4.5031667|    4.3|   14.8|2010|    6| 30|       0.0|             0.0|\n",
      "|   -2.8|   13.6|0.1823215592774815|1.1631508247068418|     9.1|3.4339872044851463| 2.302585092994046|        17.0|       77.0|       48.0|     1025.3|     1022.2|      1.0|      3.0|    4.1|   12.8|2012|    8|  1|       0.0|             0.0|\n",
      "|   -2.8|   13.9|               0.0|1.8671489542210176|7.624853|3.1354942159291497|2.0794415416798357|         4.0|       78.0|       46.0|     1035.9|     1033.7|4.4371896|4.5031667|    4.2|   12.8|2017|    6|  1|       0.0|             0.0|\n",
      "+-------+-------+------------------+------------------+--------+------------------+------------------+------------+-----------+-----------+-----------+-----------+---------+---------+-------+-------+----+-----+---+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your statement here\n",
    "show_col=num_f[:]\n",
    "show_col.append('prediction')\n",
    "show_col.append('RainTomorrow_num')\n",
    "predict.select(show_col).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10.5 Evaluate the results\n",
    "#### Step 10.5.1 \n",
    "Import MulticlassClassificationEvaluator from <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation\">pyspark.ml.evaluation</a> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10.5.2\n",
    "Using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator\">pyspark.ml.evaluation.MulticlassClassificationEvaluator</a> to evaluate the predictions from Step 10.4.4.\n",
    "Print the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.845057212530482\n"
     ]
    }
   ],
   "source": [
    "# Put your statements here\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"RainTomorrow_num\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predict)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Pipeline (10 marks)\n",
    "\n",
    "### Step 11.1\n",
    "Import <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline\">Pipeline</a> from \n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml\">pyspark.ml</a> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statement here\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11.2\n",
    "\n",
    "Rewrite Steps 9.2 to 10.5:\n",
    "<ol>\n",
    "    <li>Configure an ML pipeline which consists of the StringIndexer, OneHotEncoderEstimator, VectorAssembler, LogisticRegression</li>\n",
    "    <li>Fit the pipeline to the training data.</li>\n",
    "    <li>Make predictions on the testing data.</li>\n",
    "    <li>Evaluate the prediction results as in step 10.5.</li>\n",
    "</ol>\n",
    "\n",
    "Note: \n",
    "<ul>\n",
    "    <li>It is fine to have the steps 9.2 to 10.5 slightly rearranged in this step.</li>\n",
    "    <li>Hence, the evaluation results may be slightly different.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your statements here\n",
    "train_data,test_data=pipline_data.randomSplit([0.7,0.3])\n",
    "feature_col=[]\n",
    "for index in pipline_data.columns:\n",
    "    if pipline_data.select(index).dtypes[0][1]!='string':\n",
    "        feature_col.append(str(index))\n",
    "cat_f=[]\n",
    "for index in pipline_data.columns:\n",
    "    if pipline_data.select(index).dtypes[0][1]=='string' and index!='RainTomorrow':\n",
    "        cat_f.append(str(index))\n",
    "        feature_col.append(str(index+'_onehot'))\n",
    "string_indexers=[]\n",
    "for c in cat_f:\n",
    "    temp_ind=StringIndexer()\n",
    "    temp_ind.setInputCol(c)\n",
    "    temp_ind.setOutputCol(c+'_indexer')\n",
    "    string_indexers.append(temp_ind)\n",
    "\n",
    "encoder = OneHotEncoder(dropLast=False, inputCols=[c+'_indexer' for c in cat_f], outputCols=[c+'_onehot' for c in cat_f])\n",
    "vector=VectorAssembler(inputCols=feature_col,outputCol=\"features\")\n",
    "string_indexers.append(StringIndexer(inputCol='RainTomorrow', outputCol='RainTomorrow_num'))\n",
    "lr=LogisticRegression(regParam=0.01, maxIter=1000,labelCol = 'RainTomorrow_num',featuresCol=\"features\")\n",
    "stages_all=string_indexers+[encoder,vector,lr]\n",
    "pipeline = Pipeline(stages=stages_all).fit(train_data)\n",
    "predictions = pipeline.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"RainTomorrow_num\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12 Submission\n",
    "Submit your jupyter notebook (.ipynb) to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>The end of HW2</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
